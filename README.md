# CS5892-News-Summarization-System
Demo:https://huggingface.co/spaces/liamvbetts/bart-news-summary-v1
Poster: 
## 项目目标
- Continual Learning in News Summarization（基于持续学习的新闻摘要模型）
- 技术目标：将持续学习技术应用于T5-small模型，解决新闻摘要任务中的“灾难性遗忘”问题（模型学习新数据后丢失旧领域知识），同时适配新闻数据的“概念漂移”（如人物实体、话题、语义风格随时间变化）。
- 工程目标：搭建“数据采集-模型训练-服务部署”全链路自动化系统，实现模型日级更新与高效Web服务，降低资源消耗。
- 业务目标：提供实时新闻摘要服务，支持用户自定义检索与摘要获取，确保摘要质量与系统可用性。

## 项目概述
核心是将持续学习技术应用于 T5-small 模型以解决新闻摘要任务中的 “灾难性遗忘” 问题。项目以 CNN/DailyMail 数据集为初始训练数据，再通过 NewsAPI 采集 2024 年 2 月 13 日 - 3 月 19 日的新闻（分三个周期）进行增量训练，采用 EWC 正则化与 Rehearsal 复述策略，搭配全自动化数据处理 pipeline（Google Cloud 托管）和 Flask 交互式 Web 服务，既实现模型日级更新、缓解领域漂移影响，又支持用户实时获取新闻摘要并查看 ROUGE 评分，平衡了摘要质量、训练效率与服务可用性。

## 项目核心背景与动机
1. 新闻数据的“概念漂移”问题
新闻数据随时间变化显著，具体体现在三个维度：
- 实体与话题更新：2016年前新闻高频提及“Clinton、Obama”，当前高频提及“Biden、Trump”，同时涌现“AI大模型”“量化交易”等新话题与关键词。
- 语义与风格变化：旧新闻摘要偏向“客观陈述事件脉络”，当前新闻更强调“核心亮点抓取”，部分词汇衍生新含义（如“算力”从机械术语变为AI领域概念）。
- 领域扩展：传统数据集（如CNN/DailyMail）以通用新闻为主，当前用户需求延伸至科技、财经等垂直领域，模型需适配新语义体系。
2. 传统模型的痛点
基于旧数据训练的静态模型难以适配新新闻，会出现“灾难性遗忘”——学习新领域数据后，旧领域摘要准确率大幅下降，无法满足新闻摘要的实时性与准确性需求。

## 数据层设计
1. 数据来源
- 初始训练数据：CNN/DailyMail数据集（约30万条，代表2015年前旧新闻，为模型提供基础摘要能力）。
- 增量训练数据：通过NewsAPI采集的2024年2月13日-3月19日新闻，覆盖NYT（纽约时报）、WSJ（华尔街日报）等权威源，日均约500篇，按时间分为3个周期：
  - Period-1（2月13日-2月24日）：通用新闻（社会事件、国际动态）。
  - Period-2（2月25日-3月8日）：科技+体育新闻（垂直领域扩展）。
  - Period-3（3月9日-3月19日）：财经+本地新闻（进一步扩展领域）。
2. 数据处理 pipeline
- 全自动化流程：基于Google Cloud托管，每日定时调用NewsAPI采集数据，内置错误处理机制（遇失效链接/空内容自动跳过，避免流程中断）。
- 数据标注与格式：复用NewsAPI提供的“highlight”字段作为参考摘要，通过URL爬虫获取新闻正文，组成“正文-摘要”对；同时筛选无偏见、高口碑的新闻源，确保数据客观性。
- 数据质量保障：去重（基于标题+正文MD5）、格式统一（统一时间戳、去除广告标签），确保训练数据一致性。
## 模型层设计
1. 基础模型选型：T5-small
- 选型逻辑：平衡“效率”与“性能”。
  - 架构适配：Encoder-Decoder架构天然适合生成式摘要任务，优于仅Encoder的BERT模型。
  - 效率优势：参数约6000万，单轮增量训练仅需GPU（如A10）跑12小时，满足“日级增量训练”需求；避免大模型（如T5-base，2.2亿参数）训练成本高、周期长的问题。
  - 基础性能：在CNN/DailyMail数据集初始微调后，摘要效果达标，为后续持续学习奠定基础。
2. 持续学习技术方案
采用“正则化+复述”组合策略，解决灾难性遗忘：
- 弹性权重合并（EWC，正则化类）：计算模型在旧领域关键参数的Fisher信息矩阵，在损失函数中加入惩罚项，限制这些参数在增量训练时的变化幅度，保护旧知识（无需存储旧数据）。
- 复述策略（Rehearsal，复述类）：每轮增量训练时，混入33%旧领域数据（或DGR生成的旧领域伪样本），让模型“复习”旧知识，避免遗忘。
- 技术验证（图a、b）：
  - 图a（33%复述数据）：旧领域（CNN/DailyMail）ROUGE-1仅从0.46降至0.44（下降4.3%），新领域（Period-3）ROUGE-1从0.32提升至0.45（提升40.6%），证明复述能兼顾“保旧”与“学新”。
  - 图b（16%复述数据）：旧领域ROUGE-1降至0.39（下降15.2%），证明“足够的旧数据复述是抑制遗忘的关键”。
3. LoRA技术的应用与权衡
- 技术原理：冻结T5-small预训练权重，在Transformer层注入可训练低秩分解矩阵，减少训练参数（从6000万降至约120万）。
- 效果（图c）：GPU消耗从1764MB降至938MB（降低47%），但新领域（Period-3）ROUGE-1从0.48降至0.43（下降10.4%）。
- 选型结果：因项目核心目标是保证摘要质量，LoRA仅作为“低成本备选方案”保留，核心方案仍采用“全参数微调+EWC/Rehearsal”。

## 实验设计与结果
1. 实验核心验证点
- 验证“领域漂移”存在性：用bart-large-cnn的最后一层隐藏层生成文本嵌入，可视化显示CNN/DailyMail旧数据与NewsAPI新数据的嵌入分布显著分离，证明新旧新闻存在领域差异，持续学习有必要。
- 验证持续学习技术有效性：对比“无持续学习的增量训练模型”“EWC+Rehearsal模型”，前者旧领域ROUGE-1下降超25%，后者仅下降4.3%，证明技术能缓解灾难性遗忘。
- 验证复述数据比例影响：如前文图a、b所示，33%复述比例效果最优，16%比例会加剧遗忘。
2. 评价指标
以ROUGE为核心指标，重点关注：
- ROUGE-1：1-gram重叠率，反映摘要准确性（关键名词是否正确）。
- ROUGE-L：最长公共子序列重叠率，反映摘要完整性（是否覆盖新闻核心事件）。
六、工程落地与Web服务
1. 自动化训练与模型管理
- 训练调度：用Colab实现每日定时训练，自动加载前一日模型权重，接入当日新增数据。
- 模型管理：训练完成后，模型以“时间戳+周期名”（如“20240319-Period3”）命名，自动上传至Hugging Face Hub，同时清理本地目录（释放约10GB空间）。
- 日志监控：每轮训练生成ROUGE评分日志，同步至Google Cloud Storage，支持性能追溯。
2. Web服务（News Summary App）
- 技术栈：Flask后端+响应式前端。
- 核心功能：
  - 新闻检索与摘要：用户输入关键词（如“AI大模型”）获取相关新闻及摘要，或直接获取随机热门头条。
  - 性能可视化：实时展示生成摘要与原文“highlight”的ROUGE评分（如ROUGE-1:0.46、ROUGE-L:0.49），让用户直观了解质量。
- 响应速度保障：
  - 模型推理优化：将推理精度从fp32改为fp16，速度提升50%。
  - 缓存与扩容：缓存24小时内相同新闻的摘要结果，Google Cloud负载均衡支持高并发时自动扩容，响应时间≤3秒/次。
七、相关工作
- 正则化类：如EWC（Elastic Weight Consolidations），通过损失函数惩罚项保护旧知识。
- 复述类：如Rehearsal（混入旧数据）、Deep Generative Replay（生成旧数据伪样本），让模型复习旧知识。
- 其他技术：Gradient Episodic Memory（梯度 episodic 记忆）、Progressive Neural Networks（渐进式神经网络）。
